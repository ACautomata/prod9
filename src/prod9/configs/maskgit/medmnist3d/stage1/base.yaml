# =============================================================================
# prod9 MedMNIST 3D Stage 1 Configuration
# Complete configuration for VQGAN-style autoencoder training with FSQ
# CLI: prod9-train-autoencoder train --config <this_file>
# =============================================================================
# 
# =============================================================================
# Output paths
# =============================================================================
output_dir: "outputs/medmnist3d_stage1"
autoencoder_export_path: "outputs/medmnist3d_autoencoder.pt"
# 
# =============================================================================
# Model Architecture
# =============================================================================
model:
  # ----------------------------------------------------------------------
  # AutoencoderFSQ configuration
  # ----------------------------------------------------------------------
  autoencoder:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    levels: [8, 8, 8, 6, 5]  # 8^3 = 512 codebook size
    num_channels: [64, 128, 256, 256]  # Reduced channel sizes for memory efficiency (4 layers for 64^3 input)
    attention_levels: [False, False, False, True]  # Attention only in deepest layer
    num_res_blocks: [2, 2, 2, 2]  # Residual blocks per layer (4 layers)
    norm_num_groups: 32  # Group normalization groups
    num_splits: 2  # No splitting for small 64^3 inputs (default is 16)
    
  # ----------------------------------------------------------------------
  # MultiScalePatchDiscriminator configuration
  # ----------------------------------------------------------------------
  discriminator:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    num_d: 1  # Number of discriminators (multi-scale)
    channels: 64  # Base channel count
    num_layers_d: 3    
    kernel_size: 4              
    activation: ["LEAKYRELU", {"negative_slope": 0.2}]
    norm: "BATCH"
    minimum_size_im: 64
    
  # MedMNIST 3D doesn't need modality embeddings (single modality)
  num_modalities: 1
  contrast_embed_dim: null  # Stage 1 doesn't need contrast embeddings
    
# =============================================================================
# Training Configuration
# =============================================================================
training:
  # ----------------------------------------------------------------------
  # Optimizer settings
  # ----------------------------------------------------------------------
  optimizer:
    lr_g: 1e-4  # Generator (autoencoder) learning rate
    lr_d: 1e-4  # Discriminator learning rate (same as lr_g)
    b1: 0.5  # Adam beta1
    b2: 0.9  # Adam beta2
    weight_decay: 1e-5
    
  # ----------------------------------------------------------------------
  # Learning rate scheduler
  # ----------------------------------------------------------------------
  scheduler:
    type: "cosine"  # Options: constant, cosine, step
    T_max: 100  # For cosine
    step_size: 30  # For step
    gamma: 0.1  # For step
    eta_min: 0  # For cosine
    
  # ----------------------------------------------------------------------
  # Training loop settings
  # ----------------------------------------------------------------------
  loop:
    sample_every_n_steps: 100  # How often to generate samples during training
    
  # ----------------------------------------------------------------------
  # Training stability settings
  # ----------------------------------------------------------------------
  stability:
    grad_norm_logging: true  # Log gradient norms for monitoring training health
    warmup_enabled: true  # Enable learning rate warmup
    warmup_steps: null  # Auto-calculate based on warmup_ratio
    warmup_ratio: 0.02  # 2% of total training steps for warmup
    warmup_eta_min: 0.0
    
# =============================================================================
# Data Configuration
# =============================================================================
data:
  # ----------------------------------------------------------------------
  # MedMNIST 3D dataset configuration
  # ----------------------------------------------------------------------
  dataset_name: "organmnist3d"  # Base config: use small dataset (11 classes, ~700 samples)
  # Options: organmnist3d, nodulemnist3d, adrenalmnist3d, fracturemnist3d, vesselmnist3d, synapsemnist3d, "all"
  # Large config uses "all" to combine all 6 datasets (~6200 training samples)
  
  size: 64  # Image size: 28 or 64
  root: "./.medmnist"
  download: true
  intensity_a_min: 0.0
  intensity_a_max: 1.0
  intensity_b_min: -1.0
  intensity_b_max: 1.0
  intensity_clip: true
  
  # ----------------------------------------------------------------------
  # Data loader settings
  # ----------------------------------------------------------------------
  batch_size: 4
  num_workers: 4
  train_val_split: 0.9
  
  # ----------------------------------------------------------------------
  # Data augmentation (recommended for MedMNIST3D due to small sample size)
  # ----------------------------------------------------------------------
  augmentation:
    enabled: true
    flip_prob: 0.5
    flip_axes: [0, 1, 2]  # Axial, coronal, sagittal
    rotate_prob: 0.5
    rotate_range: 0.26  # 15 degrees (in radians)
    zoom_prob: 0.5
    zoom_min: 0.9
    zoom_max: 1.1
    shift_intensity_prob: 0.5
    shift_intensity_offset: 0.1  # +/- 10% shift
    
# =============================================================================
# Loss Configuration (aligned with Taming Transformers VQGAN best practices)
# =============================================================================
loss:
  # Loss type selector
  # ----------------------------------------------------------------------
  loss_type: "lpips"  # Options: "lpips" (default), "ffl"
  
  discriminator_iter_start: 10000  # Discriminator warmup: 10K steps (VQGAN best practice)
  
  # ----------------------------------------------------------------------
  # Reconstruction loss (L1)
  # ----------------------------------------------------------------------
  reconstruction:
    weight: 1.0  # L1 loss weight (aligned with VQGAN default)
  
  # ----------------------------------------------------------------------
  # Perceptual Loss (LPIPS-based using pretrained network)
  # ----------------------------------------------------------------------
  perceptual:
    weight: 1.0  # Perceptual loss weight (raised from 0.5 to match VQGAN)
    network_type: "alex"
    is_fake_3d: true
    fake_3d_ratio: 0.5
  
  # ----------------------------------------------------------------------
  # Focal Frequency Loss (FFL) - REQUIRED if loss_type: "ffl"
  # ----------------------------------------------------------------------
  focal_frequency:  # Only used when loss_type: "ffl"
    weight: 0.5  # FFL loss weight multiplier, default: 0.5
    alpha: 1.0  # Focusing exponent (higher = more focus on large errors), default: 1.0
    patch_factor: 1  # Split image into (patch_factor x patch_factor) patches before FFT, default: 1
    ave_spectrum: false  # Use minibatch-average spectrum, default: false
    log_matrix: false  # Apply log(1 + w) before normalization, default: false
    batch_matrix: false  # Normalize w using batch-level max, default: false
    eps: 1e-8  # Numerical stability constant, default: 1e-8
    axes: [2, 3, 4]  # Slicing axes for 3D: 2=D (axial), 3=H (coronal), 4=W (sagittal)
    ratio: 1.0  # Fraction of slices to use per axis (1.0 = all slices), default: 1.0
  
  # ----------------------------------------------------------------------
  # Adversarial loss
  # ----------------------------------------------------------------------
  adversarial:
    weight: 0.8  # Adversarial loss base weight (raised from 0.1 to match VQGAN)
    criterion: "least_squares"  # hinge, least_squares, bce
  
  # ----------------------------------------------------------------------
  # Commitment loss (FSQ) - NOTE: FSQ does not use commitment loss
  # ----------------------------------------------------------------------
  # FSQ (Finite Scalar Quantization) uses learned projection layers to quantize
  # each dimension independently to discrete scalar values in [-1, 1] range.
  # Unlike VQ, FSQ does NOT have a codebook that needs stabilization,
  # therefore commitment loss is not applicable (always 0.0).
  commitment:
    weight: 0.0  # FSQ has no codebook, so no commitment loss needed
  
  # ----------------------------------------------------------------------
  # Adaptive weight calculation
  # ----------------------------------------------------------------------
  adaptive:
    max_weight: 10000.0  # Clamp adaptive weight (VQGAN standard)
    grad_norm_eps: 0.0001  # Epsilon for gradient norm ratio
    
# =============================================================================
# Callbacks Configuration
# =============================================================================
callbacks:
  # ----------------------------------------------------------------------
  # Model checkpointing
  # ----------------------------------------------------------------------
  checkpoint:
    monitor: "val/lpips"
    mode: "min"
    save_top_k: 3
    save_last: true
    every_n_epochs: null  # null = check every epoch
    
  # ----------------------------------------------------------------------
  # Early stopping
  # ----------------------------------------------------------------------
  early_stop:
    enabled: true
    monitor: "val/lpips"
    patience: 10
    mode: "min"
    min_delta: 0.0
    check_finite: true
    
  # ----------------------------------------------------------------------
  # Learning rate monitoring
  # ----------------------------------------------------------------------
  lr_monitor: true
  
  # ----------------------------------------------------------------------
  # PyTorch Profiler (disabled by default)
  # ----------------------------------------------------------------------
  profiler:
    enabled: False
    profile_cpu: true
    profile_cuda: true
    record_shapes: true
    with_stack: true
    profile_memory: true
    trace_dir: "profiler"
    
# =============================================================================
# Trainer Configuration
# =============================================================================
trainer:
  # ----------------------------------------------------------------------
  # Training settings
  # ----------------------------------------------------------------------
  max_epochs: 100
  
  # ----------------------------------------------------------------------
  # Hardware
  # ----------------------------------------------------------------------
  hardware:
    accelerator: "auto"  # auto, gpu, cpu, mps (Apple Silicon)
    devices: "auto"       # auto, 1, 2, 4, etc.
    precision: "bf16-mixed"  # 32, 16, bf16
  
  # ----------------------------------------------------------------------
  # Logging
  # ----------------------------------------------------------------------
  logging:
    log_every_n_steps: 10
    val_check_interval: 1.0  # Validate every N epochs
    limit_train_batches: null  # null = all
    limit_val_batches: 5
    logger_version: null  # null = auto-increment
  
  # ----------------------------------------------------------------------
  # Gradient handling
  # ----------------------------------------------------------------------
  gradient_clip_val: null  # Not used for manual optimization
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  
  # ----------------------------------------------------------------------
  # Debugging/profiling
  # ----------------------------------------------------------------------
  profiler: null  # null, simple, advanced, pytorch
  detect_anomaly: false
  benchmark: false
    
# =============================================================================
# Sliding Window Inference
# =============================================================================
sliding_window:
  # Disable SW for Stage 1 (training uses direct calls)
  enabled: false
    
  # Window configuration
  roi_size: [64, 64, 64]
  overlap: 0.5  # 0-1, higher = smoother but slower
  sw_batch_size: 1  # Increase if GPU memory allows
  mode: "gaussian"  # gaussian, constant, mean
