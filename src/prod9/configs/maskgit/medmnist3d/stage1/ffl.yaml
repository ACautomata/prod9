# =============================================================================
# prod9 MedMNIST 3D Stage 1 Configuration with Focal Frequency Loss
# Complete configuration for VQGAN-style autoencoder training with FSQ
# Uses Focal Frequency Loss (FFL) instead of LPIPS for perceptual loss
# =============================================================================

# Output paths
output_dir: "outputs/medmnist3d_stage1_ffl"
autoencoder_export_path: "outputs/medmnist3d_autoencoder_ffl.pt"

# =============================================================================
# Model Architecture
# =============================================================================
model:
  # AutoencoderFSQ configuration
  autoencoder:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    levels: [8, 8, 8, 6, 5]  # 8^3 = 512 codebook size
    num_channels: [64, 128, 256, 256]  # Reduced channel sizes for memory efficiency (4 layers for 64^3 input)
    attention_levels: [False, False, False, True]  # Attention only in deepest layer
    num_res_blocks: [2, 2, 2, 2]  # Residual blocks per layer (4 layers)
    norm_num_groups: 32  # Group normalization groups
    num_splits: 2  # No splitting for small 64^3 inputs (default is 16)

  # MultiScalePatchDiscriminator configuration
  discriminator:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    num_d: 1
    channels: 64
    num_layers_d: 3
    kernel_size: 4
    activation: ["LEAKYRELU", {"negative_slope": 0.2}]
    norm: "BATCH"
    minimum_size_im: 64

  # MedMNIST 3D doesn't need modality embeddings (single modality)
  num_modalities: 1
  contrast_embed_dim: null  # Stage 1 doesn't need contrast embeddings

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Optimizer settings
  optimizer:
    lr_g: 1e-4  # Generator (autoencoder) learning rate (reduced for stability)
    lr_d: 1e-4  # Discriminator learning rate (same as lr_g)
    b1: 0.5
    b2: 0.9
    weight_decay: 1e-2

  # Learning rate scheduler (optional, currently constant)
  scheduler:
    type: "cosine"  # Options: constant, cosine, step
    T_max: 100  # For cosine
    eta_min: 0  # For cosine

  # Training loop settings
  loop:
    sample_every_n_steps: 100  # How often to generate samples during training

  # Training stability settings
  stability:
    grad_norm_logging: true  # Log gradient norms for monitoring training health
    warmup_enabled: true  # Enable learning rate warmup
    warmup_steps: null  # Auto-calculate based on warmup_ratio (set explicitly to override)
    warmup_ratio: 0.02  # 2% of total training steps for warmup
    warmup_eta_min: 0.0  # Aligned with base.yaml

# =============================================================================
# Data Configuration
# =============================================================================
data:
  # MedMNIST 3D dataset configuration
  # Option 1: Single dataset (backward compatible)
  dataset_name: "all"  # Options: organmnist3d, nodulemnist3d, adrenalmnist3d, fracturemnist3d, vesselmnist3d, synapsemnist3d, "all"
  # Option 2: Multiple datasets (combines all specified datasets)
  dataset_names: null  # null = use dataset_name, or specify list: ["organmnist3d", "nodulemnist3d", ...]

  # Use "all" to combine all 6 MedMNIST 3D datasets (~6200 training samples)
  # Uncomment the line below to enable:
  # dataset_name: "all"

  size: 64  # Image size: 28 or 64
  root: "./.medmnist"
  download: true
  intensity_a_min: 0.0
  intensity_a_max: 1.0
  intensity_b_min: -1.0
  intensity_b_max: 1.0
  intensity_clip: true

  # Data loader settings
  batch_size: 4
  num_workers: 4
  train_val_split: 0.9

  # Data augmentation (recommended for MedMNIST 3D due to small sample size)
  augmentation:
    enabled: true
    flip_prob: 0.5
    flip_axes: [0, 1, 2]  # Axial, coronal, sagittal
    rotate_prob: 0.5
    rotate_range: 0.26  # 15 degrees (in radians)
    zoom_prob: 0.5
    zoom_min: 0.9
    zoom_max: 1.1
    shift_intensity_prob: 0.5
    shift_intensity_offset: 0.1  # +/- 10% shift

# =============================================================================
# Loss Configuration
# =============================================================================
loss:
  # Loss type selector: "lpips" or "ffl"
  loss_type: "ffl"

  discriminator_iter_start: 10000  # Aligned with base.yaml

  # Reconstruction loss (L1)
  reconstruction:
    weight: 1.0

  # Perceptual Loss using Focal Frequency Loss (FFL)
  # FFL computes frequency-domain distance with adaptive weighting
  focal_frequency:
    weight: 1.0  # Aligned with base.yaml perceptual.weight
    alpha: 1.0  # Focusing exponent (higher = more focus on large errors)
    patch_factor: 1  # Split image into patches before FFT (1 = no patching)
    ave_spectrum: false  # Use minibatch-average spectrum
    log_matrix: false  # Apply log(1 + w) before normalization
    batch_matrix: false  # Normalize w using batch-level max
    eps: 1e-8  # Numerical stability constant
    # SliceWiseFake3DLoss parameters for 3D volumes
    axes: [2, 3, 4]  # Slicing axes: 2=D (axial), 3=H (coronal), 4=W (sagittal)
    ratio: 1.0  # Aligned with base.yaml: Fraction of slices to use (1.0 = all slices)

  # Fallback LPIPS config (kept for reference, not used when loss_type="ffl")
  perceptual:
    weight: 1.0  # Aligned with base.yaml (NOT used in FFL mode)
    network_type: "alex"
    is_fake_3d: true
    fake_3d_ratio: 0.5

  # Adversarial loss
  adversarial:
    weight: 0.8  # Aligned with base.yaml
    criterion: "least_squares"  # hinge, least_squares, bce

  # Commitment loss (FSQ)
  commitment:
    weight: 0.0

  # Adaptive weight calculation
  adaptive:
    max_weight: 10000.0  # Aligned with base.yaml
    grad_norm_eps: 0.0001  # Epsilon for gradient norm ratio

# =============================================================================
# Callbacks Configuration
# =============================================================================
callbacks:
  # Model checkpointing
  checkpoint:
    monitor: "val/lpips"  # FFL doesn't have a separate validation metric
    mode: "min"
    save_top_k: 3
    save_last: true
    every_n_epochs: null  # null = check every epoch

  # Early stopping
  early_stop:
    enabled: true
    monitor: "val/lpips"
    patience: 10
    mode: "min"
    min_delta: 0.0  # Minimum change to qualify as improvement
    check_finite: true  # Aligned with base.yaml

  # Learning rate monitoring
  lr_monitor: true

  # PyTorch Profiler (disabled by default)
  profiler:
    enabled: False
    profile_cpu: true
    profile_cuda: true
    record_shapes: true
    with_stack: true
    profile_memory: true
    trace_dir: "profiler"

# =============================================================================
# Trainer Configuration
# =============================================================================
trainer:
  # Training settings
  max_epochs: 100

  # Hardware
  hardware:
    accelerator: "auto"  # auto, gpu, cpu, mps (Apple Silicon)
    devices: "auto"  # auto, 1, 2, 4, etc.
    precision: "bf16-mixed"  # Aligned with base.yaml: 32, 16, bf16

  # Logging
  logging:
    log_every_n_steps: 10
    val_check_interval: 1.0  # Validate every N epochs
    limit_train_batches: null  # null = all batches
    limit_val_batches: 5
    logger_version: null  # null = auto-increment

  # Gradient handling
  gradient_clip_val: null  # Not used for manual optimization
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1

  # Debugging/profiling
  profiler: null  # null, simple, advanced, pytorch
  detect_anomaly: false
  benchmark: false
# =============================================================================
# Sliding Window Inference
# =============================================================================
sliding_window:
  # Disable SW for Stage 1 (training uses direct calls)
  enabled: false

  # Window configuration
  roi_size: [64, 64, 64]
  overlap: 0.5  # 0-1, higher = smoother but slower
  sw_batch_size: 1  # Increase if GPU memory allows
  mode: "gaussian"  # gaussian, constant, mean
