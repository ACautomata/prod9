# =============================================================================
# prod9 Transformer Stage 2 Configuration
# Complete configuration for MaskGiT transformer training
# =============================================================================

# Paths
autoencoder_path: "outputs/autoencoder_final.pt"
output_dir: "outputs/stage2"

# =============================================================================
# Model Architecture
# =============================================================================
model:
  # Transformer decoder configuration
  transformer:
    d_model: 192  # Latent token dimension (was latent_channels)
    c_model: 192  # Condition token dimension (was cond_channels)
    patch_size: 2  # 3D patch size
    hidden_dim: 512
    cond_dim: 512
    num_heads: 8
    num_blocks: 12
    codebook_size: 512  # FSQ codebook size (8*8*8 for levels=[8,8,8])
    mlp_ratio: 4.0
    dropout: 0.1

  # Class/condition embeddings for BraTS modalities
  num_classes: 4  # 4 BraTS modalities: T1, T1ce, T2, FLAIR
  contrast_embed_dim: 64

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Optimizer settings
  optimizer:
    learning_rate: 1e-4
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.01

  # Learning rate scheduler
  scheduler:
    type: "constant"  # constant, cosine, step
    # T_max: 50  # For cosine
    # step_size: 15  # For step
    # gamma: 0.1  # For step
    # eta_min: 0  # For cosine

  # Training loop
  loop:
    sample_every_n_steps: 100  # How often to generate samples during training

  # Unconditional generation probability
  unconditional:
    unconditional_prob: 0.1

# =============================================================================
# MaskGiT Sampler Configuration
# =============================================================================
sampler:
  steps: 12
  mask_value: -100
  scheduler_type: "log"  # log, linear, sqrt
  temperature: 1.0  # Sampling temperature (lower = more deterministic)

# =============================================================================
# Data Configuration
# =============================================================================
data:
  # Data paths
  data_dir: "${BRATS_DATA_DIR:data/BraTS}"

  # Modalities
  modalities: ["T1", "T1ce", "T2", "FLAIR"]

  # Data loader
  batch_size: 2
  num_workers: 4
  cache_rate: 0.5
  pin_memory: true
  train_val_split: 0.8

  # Training crop size
  roi_size: [64, 64, 64]

  # Preprocessing (same as stage 1)
  preprocessing:
    spacing: [1.0, 1.0, 1.0]
    spacing_mode: "bilinear"
    orientation: "RAS"
    intensity_a_min: 0.0
    intensity_a_max: 500.0
    intensity_b_min: -1.0
    intensity_b_max: 1.0
    clip: true

  # Augmentation (usually disabled for stage 2)
  augmentation:
    flip_prob: 0.0  # Disable for stage 2
    flip_axes: null
    rotate_prob: 0.0
    rotate_max_k: 0
    rotate_axes: [0, 1]
    shift_intensity_prob: 0.0
    shift_intensity_offset: 0.0

# =============================================================================
# Loss Configuration
# =============================================================================
loss:
  # Cross-entropy loss
  cross_entropy:
    weight: 1.0

# =============================================================================
# Callbacks Configuration
# =============================================================================
callbacks:
  checkpoint:
    monitor: "val/loss"
    mode: "min"
    save_top_k: 3
    save_last: true
    every_n_epochs: null

  early_stop:
    enabled: true
    monitor: "val/loss"
    patience: 10
    mode: "min"
    min_delta: 0.001

  lr_monitor: true

# =============================================================================
# Trainer Configuration
# =============================================================================
trainer:
  max_epochs: 50

  hardware:
    accelerator: "mps"
    devices: 1
    precision: 32

  logging:
    log_every_n_steps: 10
    val_check_interval: 1.0
    limit_train_batches: null
    limit_val_batches: 5
    logger_version: null

  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1

  profiler: null
  detect_anomaly: false
  benchmark: false

# =============================================================================
# Sliding Window Inference (REQUIRED for transformer)
# =============================================================================
sliding_window:
  enabled: true  # Always true for transformer

  roi_size: [64, 64, 64]
  overlap: 0.5
  sw_batch_size: 1
  mode: "gaussian"
