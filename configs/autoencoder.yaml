# Autoencoder Stage 1 Configuration
# Training configuration for AutoencoderFSQ with Finite Scalar Quantization

# Output directory for checkpoints and logs
output_dir: "outputs/stage1"

# Autoencoder export path (final model for Stage 2)
autoencoder_export_path: "outputs/autoencoder_final.pt"

# Model configuration
model:
  # Spatial dimensions of input images
  spatial_dims: 3

  # Input channels (1 for single modality, 4 for multi-modal)
  in_channels: 1

  # Output channels (reconstruction)
  out_channels: 1

  # FSQ quantization levels per dimension
  # Product should be >= vocabulary size needed
  levels: [8, 8, 8]  # 8^3 = 512 codebook size

  # Number of channels in encoder/decoder (last channel count)
  num_channels: 512

  # Attention levels for each encoder/decoder stage
  # False = no attention, True = with attention
  attention_levels: [False, False, True, True, True]

  # Number of residual blocks per stage
  num_res_blocks: [1, 1, 1, 1, 1]

# Training configuration
training:
  # Generator (autoencoder) learning rate
  lr_g: 1e-4

  # Discriminator learning rate
  lr_d: 4e-4

  # Adam optimizer beta1
  b1: 0.5

  # Adam optimizer beta2
  b2: 0.999

  # Weight decay (L2 regularization)
  weight_decay: 1e-5

  # Gradient clipping value (0 to disable)
  grad_clip: 1.0

  # Reconstruction loss weight
  recon_weight: 1.0

  # Perceptual loss weight
  perceptual_weight: 0.5

  # Adversarial loss weight
  adv_weight: 0.1

  # FSQ commitment loss weight
  commitment_weight: 0.25

  # Number of training epochs
  max_epochs: 100

# Discriminator configuration (for adversarial training)
discriminator:
  # Number of channels in discriminator
  fmaps: [64, 128, 256, 512]

  # Number of discriminator layers
  depth: 4

  # Discriminator learning rate
  learning_rate: 1e-4

  # Loss weight for adversarial loss
  lambda_adv: 0.1

# Sliding window configuration (for inference/validation)
sliding_window:
  # ROI size for sliding window inference
  # Should be <= training roi_size to match training distribution
  roi_size: [64, 64, 64]

  # Overlap between adjacent windows (0-1)
  # Higher = smoother transitions but slower
  overlap: 0.5

  # Number of windows to process simultaneously
  # Increase if GPU memory allows
  sw_batch_size: 1

  # Blending mode: 'gaussian', 'constant', or 'mean'
  mode: gaussian

  # Enable SW for validation (training always uses direct calls)
  enabled: false

# Data configuration
data:
  # Path to BraTS dataset (use BRATS_DATA_DIR env var, default: data/BraTS)
  data_dir: "${BRATS_DATA_DIR:data/BraTS}"

  # Batch size (adjust based on GPU memory)
  batch_size: 2

  # Number of data loader workers
  num_workers: 4

  # Cache rate for MONAI cache (0-1)
  # 1.0 = cache all data in RAM
  cache_rate: 0.5

  # ROI size for training crops (data loader provides this size)
  roi_size: [64, 64, 64]

  # Train/validation split ratio
  train_val_split: 0.8

# Loss configuration
loss:
  # Pixel-wise loss weight (L1 + SSIM)
  lambda_pixel: 1.0

  # Perceptual loss weight (VGG features if available)
  lambda_perceptual: 0.1

# Callbacks configuration
callbacks:
  # Model checkpoint
  checkpoint:
    monitor: "val/combined_metric"
    mode: "max"
    save_top_k: 3

  # Early stopping
  early_stop:
    monitor: "val/combined_metric"
    patience: 10
    mode: "max"

  # Learning rate monitor
  lr_monitor: true

# Trainer configuration
trainer:
  # Accelerator: 'gpu', 'cpu', 'mps' (Apple Silicon)
  accelerator: "mps"

  # Devices to use
  devices: 1

  # Precision: 32, 16, or bf16
  precision: 32

  # Number of validation batches to log
  limit_val_batches: 5

  # Log every N training batches
  log_every_n_steps: 10
