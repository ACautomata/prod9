# Transformer Stage 2 Configuration
# Training configuration for MaskGiT transformer

# Path to trained Stage 1 autoencoder
autoencoder_path: "outputs/autoencoder_final.pt"

# Output directory for checkpoints and logs
output_dir: "outputs/stage2"

# Model configuration
model:
  # Latent channels from Stage 1 autoencoder
  # Calculated as: product of latent shape * number of FSQ vectors
  # For latent_shape [4,4,4] with 3 FSQ vectors: 4*4*4*3 = 192
  latent_channels: 192

  # Condition channels (same as latent_channels)
  cond_channels: 192

  # Patch size for 3D transformer (splits latent into patches)
  patch_size: 2

  # Transformer dimension
  hidden_dim: 512

  # Condition embedding dimension
  cond_dim: 512

  # Number of attention heads
  num_heads: 8

  # Number of transformer blocks
  num_blocks: 12

  # MLP expansion ratio (hidden_dim * mlp_ratio = FFN dimension)
  mlp_ratio: 4.0

  # Dropout rate
  dropout: 0.1

# Training configuration
training:
  # Learning rate
  learning_rate: 1e-4

  # AdamW optimizer beta1
  beta1: 0.9

  # AdamW optimizer beta2
  beta2: 0.999

  # Weight decay
  weight_decay: 0.01

  # Gradient clipping value (0 to disable)
  grad_clip: 1.0

  # Number of training epochs
  max_epochs: 50

# MaskGiT sampler configuration
sampler:
  # Number of sampling steps
  steps: 12

  # Mask token value
  mask_value: -100

  # Scheduler type: 'log', 'linear', or 'sqrt'
  scheduler_type: log

  # Temperature for sampling (lower = more deterministic)
  temperature: 1.0

# Sliding window configuration (for autoencoder during generation)
sliding_window:
  # ROI size for sliding window inference
  # Should be <= training roi_size to match training distribution
  roi_size: [64, 64, 64]

  # Overlap between adjacent windows (0-1)
  overlap: 0.5

  # Number of windows to process simultaneously
  sw_batch_size: 1

  # Blending mode: 'gaussian', 'constant', or 'mean'
  mode: gaussian

# Data configuration
data:
  # Path to BraTS dataset (use BRATS_DATA_DIR env var, default: data/BraTS)
  data_dir: "${BRATS_DATA_DIR:data/BraTS}"

  # Batch size (adjust based on GPU memory)
  batch_size: 2

  # Number of data loader workers
  num_workers: 4

  # Cache rate for MONAI cache (0-1)
  cache_rate: 0.5

  # ROI size for training crops
  roi_size: [64, 64, 64]

  # Train/validation split ratio
  train_val_split: 0.8

# Loss configuration
loss:
  # Cross-entropy loss weight
  lambda_ce: 1.0

# Callbacks configuration
callbacks:
  # Model checkpoint
  checkpoint:
    monitor: "val/loss"
    mode: "min"
    save_top_k: 3

  # Early stopping
  early_stop:
    monitor: "val/loss"
    patience: 10
    mode: "min"

  # Learning rate monitor
  lr_monitor: true

# Trainer configuration
trainer:
  # Accelerator: 'gpu', 'cpu', 'mps' (Apple Silicon)
  accelerator: "mps"

  # Devices to use
  devices: 1

  # Precision: 32, 16, or bf16
  precision: 32

  # Number of validation batches to log
  limit_val_batches: 5

  # Log every N training batches
  log_every_n_steps: 10
